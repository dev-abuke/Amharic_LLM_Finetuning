{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/abubeker_shamil/Amharic_LLM_Finetuning'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.clean_data import DataCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.read_csv(\"data/raw/summary_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "clean_text       0\n",
       "clean_summary    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(ሲ.ኤን.ኤን.) - ከሉዊዚያና 2 ሚሊዮን የባህር ዳርቻ ነዋሪዎች መካከል...</td>\n",
       "      <td>አዲስ፡ የሉዊዚያና ገዥ፡ በግምት 10,000 የሚገመተው በኒው ኦርሊንስ ው...</td>\n",
       "      <td>ሲኤንኤን ከሉዊዚያና 2 ሚሊዮን የባህር ዳርቻ ነዋሪዎች መካከል 95 በመቶ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(RollingStone.com) -- ቦብ ዲላን \"ቴምፔስት\" የተባለውን 35...</td>\n",
       "      <td>\"ቴምፕስት\" የቦብ ዲላን 35ኛ የስቱዲዮ አልበም ነው።\\nየርዕስ ዱካ የ1...</td>\n",
       "      <td>ቦብ ዲላን ቴምፔስት የተባለውን 35ኛው የስቱዲዮ አልበሙን ከሴፕቴምበር 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ዳቮስ፣ ስዊዘርላንድ (ሲ.ኤን.ኤን) - የአውሮፓ ህብረት የውስጥ ገበያ ኮ...</td>\n",
       "      <td>የአውሮፓ ኮሚሽነር ሚሼል ባርኒየር የታቀደውን \"ሮቢን ሁድ\" ቀረጥ አጽድቀ...</td>\n",
       "      <td>ዳቮስ ስዊዘርላንድ ሲኤንኤን የአውሮፓ ህብረት የውስጥ ገበያ ኮሚሽነር በፋ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(ሲ.ኤን.ኤን.) የኒውዮርክ ግዛት ሴናተር እንዳሉት አንዳንድ የኒውዮርክ ...</td>\n",
       "      <td>ራዲዮዎች በጣም የተሳሳቱ አንዳንድ መኮንኖች ለመገናኘት የሞባይል ስልኮችን...</td>\n",
       "      <td>ሲኤንኤን የኒውዮርክ ግዛት ሴናተር እንዳሉት አንዳንድ የኒውዮርክ ፖሊስ መ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>እስራት፡ የ31 አመቱ ዊልያም ሊንዳወር ከተፋታ ከአንድ አመት በኋላ የቤተ...</td>\n",
       "      <td>ዊልያም ሊንዳወር በማክሰኞ ጠዋት በአርቫዳ ፣ ኮሎራዶ የሚገኘው ቤት በእሳ...</td>\n",
       "      <td>እስራት የ31 አመቱ ዊልያም ሊንዳወር ከተፋታ ከአንድ አመት በኋላ የቤተሰ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  (ሲ.ኤን.ኤን.) - ከሉዊዚያና 2 ሚሊዮን የባህር ዳርቻ ነዋሪዎች መካከል...   \n",
       "1  (RollingStone.com) -- ቦብ ዲላን \"ቴምፔስት\" የተባለውን 35...   \n",
       "2  ዳቮስ፣ ስዊዘርላንድ (ሲ.ኤን.ኤን) - የአውሮፓ ህብረት የውስጥ ገበያ ኮ...   \n",
       "3  (ሲ.ኤን.ኤን.) የኒውዮርክ ግዛት ሴናተር እንዳሉት አንዳንድ የኒውዮርክ ...   \n",
       "4  እስራት፡ የ31 አመቱ ዊልያም ሊንዳወር ከተፋታ ከአንድ አመት በኋላ የቤተ...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  አዲስ፡ የሉዊዚያና ገዥ፡ በግምት 10,000 የሚገመተው በኒው ኦርሊንስ ው...   \n",
       "1  \"ቴምፕስት\" የቦብ ዲላን 35ኛ የስቱዲዮ አልበም ነው።\\nየርዕስ ዱካ የ1...   \n",
       "2  የአውሮፓ ኮሚሽነር ሚሼል ባርኒየር የታቀደውን \"ሮቢን ሁድ\" ቀረጥ አጽድቀ...   \n",
       "3  ራዲዮዎች በጣም የተሳሳቱ አንዳንድ መኮንኖች ለመገናኘት የሞባይል ስልኮችን...   \n",
       "4  ዊልያም ሊንዳወር በማክሰኞ ጠዋት በአርቫዳ ፣ ኮሎራዶ የሚገኘው ቤት በእሳ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  ሲኤንኤን ከሉዊዚያና 2 ሚሊዮን የባህር ዳርቻ ነዋሪዎች መካከል 95 በመቶ...  \n",
       "1  ቦብ ዲላን ቴምፔስት የተባለውን 35ኛው የስቱዲዮ አልበሙን ከሴፕቴምበር 1...  \n",
       "2  ዳቮስ ስዊዘርላንድ ሲኤንኤን የአውሮፓ ህብረት የውስጥ ገበያ ኮሚሽነር በፋ...  \n",
       "3  ሲኤንኤን የኒውዮርክ ግዛት ሴናተር እንዳሉት አንዳንድ የኒውዮርክ ፖሊስ መ...  \n",
       "4  እስራት የ31 አመቱ ዊልያም ሊንዳወር ከተፋታ ከአንድ አመት በኋላ የቤተሰ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean text\n",
    "summary[\"clean_text\"] = summary[\"text\"].apply(lambda x: DataCleaner().remove_emojis(x))\n",
    "summary[\"clean_text\"] = summary[\"clean_text\"].apply(lambda x: DataCleaner().remove_hashtags(x))\n",
    "summary[\"clean_text\"] = summary[\"clean_text\"].apply(lambda x: DataCleaner().remove_mentions(x))\n",
    "summary[\"clean_text\"] = summary[\"clean_text\"].apply(lambda x: DataCleaner().remove_symbols(x))\n",
    "summary[\"clean_text\"] = summary[\"clean_text\"].apply(lambda x: DataCleaner().remove_hyperlinks(x))\n",
    "summary['clean_text'] = summary[\"clean_text\"].apply(lambda x: DataCleaner().normalize_char_level_missmatch(x))\n",
    "summary['clean_text'] = summary[\"clean_text\"].apply(lambda x: DataCleaner().remove_english_characters(x))\n",
    "summary['clean_text'] = summary[\"clean_text\"].apply(lambda x: DataCleaner().remove_punc_and_special_chars(x))\n",
    "summary[\"clean_text\"] = summary[\"clean_text\"].apply(lambda x: DataCleaner().remove_newline_and_extra_space(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cleaned_df = pd.DataFrame(summary[\"clean_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_summary\n",
    "summary[\"clean_summary\"] = summary[\"summary\"].apply(lambda x: DataCleaner().remove_emojis(x))\n",
    "summary[\"clean_summary\"] = summary[\"clean_summary\"].apply(lambda x: DataCleaner().remove_hashtags(x))\n",
    "summary[\"clean_summary\"] = summary[\"clean_summary\"].apply(lambda x: DataCleaner().remove_mentions(x))\n",
    "summary[\"clean_summary\"] = summary[\"clean_summary\"].apply(lambda x: DataCleaner().remove_symbols(x))\n",
    "summary[\"clean_summary\"] = summary[\"clean_summary\"].apply(lambda x: DataCleaner().remove_hyperlinks(x))\n",
    "summary['clean_summary'] = summary[\"clean_summary\"].apply(lambda x: DataCleaner().normalize_char_level_missmatch(x))\n",
    "summary['clean_summary'] = summary[\"clean_summary\"].apply(lambda x: DataCleaner().remove_english_characters(x))\n",
    "summary['clean_summary'] = summary[\"clean_summary\"].apply(lambda x: DataCleaner().remove_punc_and_special_chars(x))\n",
    "summary[\"clean_summary\"] = summary[\"clean_summary\"].apply(lambda x: DataCleaner().remove_newline_and_extra_space(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cleaned_df['clean_summary'] = summary[\"clean_summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ሲኤንኤን ከሉዊዚያና 2 ሚሊዮን የባህር ዳርቻ ነዋሪዎች መካከል 95 በመቶ...</td>\n",
       "      <td>አዲስ የሉዊዚያና ገዥ በግምት 10000 የሚገመተው በኒው ኦርሊንስ ውስጥ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ቦብ ዲላን ቴምፔስት የተባለውን 35ኛው የስቱዲዮ አልበሙን ከሴፕቴምበር 1...</td>\n",
       "      <td>ቴምፕስት የቦብ ዲላን 35ኛ የስቱዲዮ አልበም ነው የርእስ ዱካ የ14 ደቂ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ዳቮስ ስዊዘርላንድ ሲኤንኤን የአውሮፓ ህብረት የውስጥ ገበያ ኮሚሽነር በፋ...</td>\n",
       "      <td>የአውሮፓ ኮሚሽነር ሚሼል ባርኒየር የታቀደውን ሮቢን ሁድ ቀረጥ አፅድቀዋል...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ሲኤንኤን የኒውዮርክ ግዛት ሴናተር እንዳሉት አንዳንድ የኒውዮርክ ፖሊስ መ...</td>\n",
       "      <td>ራዲዮዎች በጣም የተሳሳቱ አንዳንድ መኮንኖች ለመገናኘት የሞባይል ስልኮችን...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>እስራት የ31 አመቱ ዊልያም ሊንዳወር ከተፋታ ከአንድ አመት በኋላ የቤተሰ...</td>\n",
       "      <td>ዊልያም ሊንዳወር በማክሰኞ ጠዋት በአርቫዳ ኮሎራዶ የሚገኘው ቤት በእሳት ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  \\\n",
       "0  ሲኤንኤን ከሉዊዚያና 2 ሚሊዮን የባህር ዳርቻ ነዋሪዎች መካከል 95 በመቶ...   \n",
       "1  ቦብ ዲላን ቴምፔስት የተባለውን 35ኛው የስቱዲዮ አልበሙን ከሴፕቴምበር 1...   \n",
       "2  ዳቮስ ስዊዘርላንድ ሲኤንኤን የአውሮፓ ህብረት የውስጥ ገበያ ኮሚሽነር በፋ...   \n",
       "3  ሲኤንኤን የኒውዮርክ ግዛት ሴናተር እንዳሉት አንዳንድ የኒውዮርክ ፖሊስ መ...   \n",
       "4  እስራት የ31 አመቱ ዊልያም ሊንዳወር ከተፋታ ከአንድ አመት በኋላ የቤተሰ...   \n",
       "\n",
       "                                       clean_summary  \n",
       "0  አዲስ የሉዊዚያና ገዥ በግምት 10000 የሚገመተው በኒው ኦርሊንስ ውስጥ ...  \n",
       "1  ቴምፕስት የቦብ ዲላን 35ኛ የስቱዲዮ አልበም ነው የርእስ ዱካ የ14 ደቂ...  \n",
       "2  የአውሮፓ ኮሚሽነር ሚሼል ባርኒየር የታቀደውን ሮቢን ሁድ ቀረጥ አፅድቀዋል...  \n",
       "3  ራዲዮዎች በጣም የተሳሳቱ አንዳንድ መኮንኖች ለመገናኘት የሞባይል ስልኮችን...  \n",
       "4  ዊልያም ሊንዳወር በማክሰኞ ጠዋት በአርቫዳ ኮሎራዶ የሚገኘው ቤት በእሳት ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_cleaned_df.to_csv(\"data/raw/summary_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ከ 1973 ጀምሮ, ስቲቭ ፉለር የሎውስቶን የክረምት ጠባቂ ሆኖ ሰርቷል.\n",
      "ሥራው የሚጀምረው በኖቬምበር መጀመሪያ ላይ ሲሆን ማረሻዎች ወደ ማረፊያው ሲደርሱ ያበቃል.\n",
      "በክረምቱ ወቅት እሱ ከቅርብ ጎረቤቶቹ ርቆ የሁለት ሰዓት የበረዶ ተሽከርካሪ ይጓዛል።\n",
      "በፓርኩ ውስጥ ከሚገኙ 100 ህንፃዎች ላይ በረዶውን በማጽዳት ቀናትን ያሳልፋል።\n",
      "የሱ ሌሊቶች ቲቪ በሌለበት ጎጆ ውስጥ ያሳልፋሉ፣ ያነባሉ።\n",
      "በእረፍት ጊዜው፣ አገር አቋራጭ የበረዶ መንሸራተቻ እና የክረምቱን ገጽታ ፎቶ ማንሳት ይወዳል።\n",
      "ከ 1973 ጀምሮ ስቲቭ ፉለር የሎውስቶን የክረምት ጠባቂ ሆኖ ሰርቷል ስራው የሚጀምረው በኖቬምበር መጀመሪያ ላይ ሲሆን ማረሻዎች ወደ ማረፊያው ሲደርሱ ያበቃል በክረምቱ ወቅት እሱ ከቅርብ ጎረቤቶቹ ርቆ የሁለት ሰአት የበረዶ ተሽከርካሪ ይጓዛል በፓርኩ ውስጥ ከሚገኙ 100 ህንፃዎች ላይ በረዶውን በማፅዳት ቀናትን ያሳልፋል የሱ ሌሊቶች ቲቪ በሌለበት ጎጆ ውስጥ ያሳልፋሉ ያነባሉ በእረፍት ጊዜው አገር አቋራጭ የበረዶ መንሸራተቻ እና የክረምቱን ገፅታ ፎቶ ማንሳት ይወዳል\n"
     ]
    }
   ],
   "source": [
    "print(summary['summary'][150])\n",
    "print(summary['clean_summary'][150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataCleaner().handle_missing_values(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataCleaner().export_to_csv(df=summary, file_name=\"data/raw/summary_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, load_metric, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Samuael/llama-2-7b-tebot-amharic\"  # Replace with your chosen model name\n",
    "# \"rasyosef/bert-amharic-tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_summaries, val_summaries = train_test_split(\n",
    "    summary['clean_text'], summary['clean_summary'], test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(list(train_texts), padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tokenizer(train_summaries.astype(str).tolist(), padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tokenizer(list(val_summaries), padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_encodings = tokenizer(list(val_texts), padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to tensors manually if not already in the expected format\n",
    "import torch\n",
    "\n",
    "train_encodings = {\n",
    "    'input_ids': torch.tensor(train_encodings['input_ids']),\n",
    "    'attention_mask': torch.tensor(train_encodings['attention_mask'])\n",
    "}\n",
    "\n",
    "train_labels = {\n",
    "    'input_ids': torch.tensor(train_labels['input_ids']),\n",
    "    'attention_mask': torch.tensor(train_labels['attention_mask'])\n",
    "}\n",
    "\n",
    "val_encodings = {\n",
    "    'input_ids': torch.tensor(val_encodings['input_ids']),\n",
    "    'attention_mask': torch.tensor(val_encodings['attention_mask'])\n",
    "}\n",
    "\n",
    "val_labels = {\n",
    "    'input_ids': torch.tensor(val_labels['input_ids']),\n",
    "    'attention_mask': torch.tensor(val_labels['attention_mask'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tokenized data to Huggingface Datasets\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'labels': train_labels['input_ids']  # Assuming you are using a language model for summarization\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'input_ids': val_encodings['input_ids'],\n",
    "    'attention_mask': val_encodings['attention_mask'],\n",
    "    'labels': val_labels['input_ids']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine into a DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.get('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a sample from the dataset\n",
    "print(dataset['train'][0])\n",
    "print(dataset['validation'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_int8_training\n",
    "\n",
    "# Load the base model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Prepare the model for int8 training (optional, but helps reduce memory usage)\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Tokenize example input\n",
    "input_text = \"ሰላም እንዴት ነህ?\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2, \n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],  # Assuming `dataset` is your prepared dataset\n",
    "    eval_dataset=dataset['validation']\n",
    ")\n",
    "\n",
    "def train_save_test():\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    model.save_pretrained(\"./finetuned_model\")\n",
    "\n",
    "    # Test the fine-tuned model\n",
    "    output = model.generate(**inputs, max_length=50)\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
